{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Article Generation(Text) with RNN/LSTM(Tutorial)\n",
    "## Final project for phase II - EIP at MLBLR.com\n",
    "\n",
    "**Objective:** To generate meaningful text based on a subject/entity using a recurrent neural network. \n",
    "\n",
    "Project based on \"Generate realistic Yelp reviews with Keras\" by Tony607\n",
    "[[link]](https://github.com/Tony607/Yelp_review_generation)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. <a href='#1'>Introduction</a>\n",
    "2. <a href='#2'>RNNs and how+why they work</a>\n",
    "3. <a href='#3'>Why RNN's don't work</a>\n",
    "4. <a href='#4'>LSTM</a>\n",
    "5. <a href='#5'>How+Why LSTM works</a>\n",
    "6. <a href='#6'>Interesting applications</a>\n",
    "7. <a href='#7'>Example with code</a>\n",
    "    **Text generation on countries/articles from Wiikipedia using LSTM(Keras)**\n",
    "     1. Gathering dataset\n",
    "     2. Data cleaning\n",
    "     3. Vectorizing words\n",
    "     4. Defining helper functions\n",
    "     5. Defining model\n",
    "     6. Training\n",
    "     7. Generate text\n",
    "8. <a href='#8'>Summary</a>\n",
    "9. <a href='#9'>Additional reading material</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<a id='1'></a>\n",
    "\n",
    "Traditional neural networks can’t captare temporal dependencies i.e. dependencies that vary over time, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\n",
    "\n",
    "Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. Recurrent Neural Networks were created in the 1980’s but have just been recently gaining popularity from advances to the networks designs and increased computational power from graphic processing units. They’re especially useful with sequential data because each neuron or unit can use its internal memory to maintain information about the previous input. This is great because in cases of language, “I had washed my house” is much more different than “I had my house washed”. This allows the network to gain a deeper understanding of the statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNNs and how+why they work\n",
    "<a id='2'></a>\n",
    "\n",
    "The general structure of the RNN is shown in the image below.\n",
    "![RNN_unfolded](https://cdn-images-1.medium.com/max/1600/1*NKhwsOYNUT5xU7Pyf6Znhg.png)\n",
    "\n",
    "--------------**FOLDED**---------------------------------------------------------------------**UNFOLDED**---------------------------------------------------------------------------\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "**Folded** - In the above diagram, a chunk of neural network, A, looks at some input xt and outputs a value ht. A loop allows information to be passed from one step of the network to the next.A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor.\n",
    "\n",
    "**Unfolded** - That sequential information is preserved in the recurrent network’s hidden state, which manages to span many time steps as it cascades forward to affect the processing of each new example. It is finding correlations between events separated by many moments, and these correlations are called “long-term dependencies”, because an event downstream in time depends upon, and is a function of, one or more events that came before. One way to think about RNNs is this: they are a way to share weights over time. The decision a recurrent net reached at time step t-1 affects the decision it will reach one moment later at time step t. So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life. The chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data. The arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell.\n",
    "\n",
    "**Why RNNs?**\n",
    "\n",
    "Since RNNs preserve temporal information they can be used for applications that vary with time. There are many different applications of RNNs. A great application is in collaboration with Natural Language Processing (NLP). RNNs have been demonstrated by many people on the internet who created amazing models that can represent a language model. These language models can take input such as a large set of shakespeares poems, and after training these models they can generate their own Shakespearean poems that are very hard to differentiate from originals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why RNNs don't work\n",
    "<a id='3'></a>\n",
    "\n",
    "When training long texts or a lenghty video, the results from a vanilla RNN are not that good. The problem comes from the fact that at each time step during training we are using the same weights to calculate y_t. That multiplication is also done during back-propagation. The further we move backwards, the bigger or smaller our error signal becomes. This means that the network experiences difficulty in memorising words from far away in the sequence and makes predictions based on only the most recent ones.\n",
    "\n",
    "![RNN_fail](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)\n",
    "\n",
    "                (fig) The RNN cannot learn from the hidden state at the beginning of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM\n",
    "<a id='4'></a>\n",
    "\n",
    "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn.  LSTM module can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.\n",
    "\n",
    "![LSTM](https://cdn-images-1.medium.com/max/1600/1*J5W8FrASMi93Z81NlAui4w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How+Why LSTM works\n",
    "<a id='5'></a>\n",
    "\n",
    "LSTM comprises of 4 components. They are\n",
    "\n",
    "### i. Forget gate\n",
    "The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n",
    "\n",
    "![forget_gate](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
    "\n",
    "### ii. Remember gate\n",
    "The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n",
    "\n",
    "![remember_gate](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
    "\n",
    "### iii. Learn gate\n",
    "It’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it.\n",
    "\n",
    "We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "![learn_gate](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
    "\n",
    "### iv. Use gate\n",
    "Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "![use_gate](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
    "\n",
    "**The Why**\n",
    "\n",
    "Since in LSTM, there is an additional parameter C which holds temporal information from earlier stages in the network that is needed in the later stages of the network. This gives anedge over the vanilla RNN by a large margin. Though LSTMs are quite difficult to understand, their implementation is pretty easy and the results are relatable to the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interesting applications\n",
    "<a id='6'></a> \n",
    "\n",
    " - Are you into gaming and bots? Check out the [DotA 2 bot by Open AI](https://blog.openai.com/dota-2/)\n",
    " - How about automatically [adding sounds to silent movies?](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)\n",
    " - Here is a cool tool for [automatic handwriting generation](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3)\n",
    " - Amazon's voice to text using [high quality speech recognition, Amazon Lex.](https://aws.amazon.com/lex/faqs/)\n",
    " - Facebook uses RNN and LSTM technologies for [building language models](https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/)\n",
    " - Netflix also uses RNN models - [here is an interesting read](https://arxiv.org/pdf/1511.06939.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example with code\n",
    "<a id='7'></a>\n",
    "\n",
    "## Text generation on countries/articles from Wiikipedia using LSTM(Keras)\n",
    "The following implementation is an example of text generation with LSTM. We will take the summary of a Wikipedia articles of countries and generate text based on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Gathering dataset\n",
    "\n",
    "**Current objective:** To get text from the wikipedia page and store it as as csv.\n",
    "\n",
    "The first step is to gather text data for training the model. Wikipedia has an entire dump of all articles available as a zip format. But this contains text of topics belonging to all categories. We are specifically interested in country articles alone. So a new dataset has to be created for this purpose.\n",
    "\n",
    "Wikipedia has a python library which allows for easy text scraping from the article page. It can be limited to only title or summary or even the entire page.\n",
    "\n",
    "**Link to Wikipedia library documentation:**[[link]](https://pypi.org/project/wikipedia/)\n",
    "\n",
    "Below code cell extracts summary of countries in the list and saves them as a csv file. It takes about 1-2 seconds per title depending on the internet speed.\n",
    "\n",
    "**Note 1:** If needed to train on any other data apart from countries, replace the list of countries with that category required to train\n",
    "\n",
    "**Note 2:** The following cell needs to be run only once for the first time when gathering data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports \n",
    "import wikipedia  \n",
    "from tqdm import tqdm # to print current progress status\n",
    "import csv\n",
    "\n",
    "## Create a empty dictionary to store the descriptions\n",
    "summary_dict={}\n",
    "\n",
    "## create a list of article headings that the model needs to be trained on\n",
    "countries_list=[\"Afghanistan\",\"Albania\",\"Algeria\",\"Andorra\",\"Angola\",\"Anguilla\",\"Antigua & Barbuda\",\n",
    "\"Argentina\",\"Armenia\",\"Australia\",\"Austria\",\"Azerbaijan\",\"Bahamas\",\"Bahrain\",\"Bangladesh\",\"Barbados\",\n",
    "\"Belarus\",\"Belgium\",\"Belize\",\"Benin\",\"Bermuda\",\"Bhutan\",\"Bolivia\",\"Bosnia & Herzegovina\",\"Botswana\",\n",
    "\"Brazil\",\"Brunei Darussalam\",\"Bulgaria\",\"Burkina Faso\",\"Burma\",\"Burundi\",\"Cambodia\",\"Cameroon\",\n",
    "\"Canada\",\"Cape Verde\",\"Cayman Islands\",\"Central African Republic\",\"Chad\",\"Chile\",\"China\",\"Colombia\",\"Comoros\",\n",
    "\"Democratic Republic of the Congo\",\"Costa Rica\",\"Croatia\",\"Cuba\",\"Cyprus\",\"Czech Republic\",\n",
    "\"Denmark\",\"Djibouti\",\"Dominica\",\"Dominican Republic\",\"Ecuador\",\"Egypt\",\n",
    "\"El Salvador\",\"Equatorial Guinea\",\"Eritrea\",\"Estonia\",\"Ethiopia\",\"Fiji\",\"Finland\",\"France\",\"French Guiana\",\"Gabon\",\"Gambia\",\n",
    "\"Democratic Republic of Georgia\",\"Germany\",\"Ghana\",\"Great Britain\",\"Greece\",\"Grenada\",\"Guadeloupe\",\n",
    "\"Guatemala\",\"Guinea\",\"Guinea-Bissau\",\"Guyana\",\"Haiti\",\"Honduras\",\"Hungary\",\"Iceland\",\"India\",\n",
    "\"Indonesia\",\"Iran\",\"Iraq\",\"Israel and the Occupied Territories\",\"Italy\",\"Ivory Coast\",\"Jamaica\",\"Japan\",\"Jordan\",\n",
    "\"Kazakhstan\",\"Kenya\",\"Kosovo\",\"Kuwait\",\"Kyrgyzstan\",\"Laos\",\"Latvia\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Libya\",\"Liechtenstein\",\"Lithuania\",\n",
    "\"Luxembourg\",\"Republic of Macedonia\",\"Madagascar\",\"Malawi\",\"Malaysia\",\"Maldives\",\"Mali\",\"Malta\",\n",
    "\"Martinique\",\"Mauritania\",\"Mauritius\",\"Mayotte\",\"Mexico\",\"Monaco\",\"Mongolia\",\"Montenegro\",\n",
    "\"Montserrat\",\"Morocco\",\"Mozambique\",\"Namibia\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"North Korea\",\"Norway\",\n",
    "\"Oman\",\"Pacific Islands\",\"Pakistan\",\"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\n",
    "\"Portugal\",\"Puerto Rico\",\"Qatar\",\"Romania\",\"Russian Federation\",\n",
    "\"Rwanda\",\"Saint Kitts and Nevis\",\"Saint Lucia\",\"Saint Vincents and the Grenadines\",\"Samoa\",\"Sao Tome and Principe\",\n",
    "\"Saudi Arabia\",\"Senegal\",\"Serbia\",\"Seychelles\",\"Sierra Leone\",\n",
    "\"Singapore\",\"Slovakia\",\"Slovenia\",\"Solomon Islands\",\"Somalia\",\"South Africa\",\"South Korea\",\"South Sudan\",\n",
    "\"Spain\",\"Sri Lanka\",\"Sudan\",\"Suriname\",\"Swaziland\",\"Sweden\",\"Switzerland\",\"Syria\",\n",
    "\"Tajikistan\",\"Tanzania\",\"Thailand\",\"Timor Leste\",\"Togo\",\"Trinidad & Tobago\",\"Tunisia\",\"Turkey\",\n",
    "\"Turkmenistan\",\"Turks & Caicos Islands\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United States of America\",\n",
    "\"Uruguay\",\"Uzbekistan\",\"Venezuela\",\"Vietnam\",\"Virgin Islands (UK)\",\"Virgin Islands (US)\",\"Yemen\",\n",
    "\"Zambia\",\"Zimbabwe\"]\n",
    "\n",
    "## loop through each title to get the summary of the country and store them in the dictionary created earlier\n",
    "for country in tqdm(countries_list):\n",
    "    summ=wikipedia.summary(country)\n",
    "    summary_dict[country]=summ\n",
    "\n",
    "## create a csv file with the data in the dictionary\n",
    "with open('country_desc.csv','w', encoding='utf8') as f:\n",
    "    heads=[\"country\", \"description\"]\n",
    "    w=csv.DictWriter(f,heads)\n",
    "    w.writeheader()\n",
    "    for key, val in sorted(summary_dict.items()):\n",
    "            row = {'country': key, 'description':val}\n",
    "            w.writerow(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Data cleaning\n",
    "**Currect objective:** To clean and preprocess the text that is capable of being fed into the network and store it in a txt file\n",
    "\n",
    "Now that we have our data, we need to clean it and preprocess it. The data in the csv is in raw form and it needs to be processed before it can be fed to the network. This process is donw in a number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (195, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afghanistan ( ( listen); Pashto/Dari: افغانستا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Albania ( ( listen) a(w)l-BAY-nee-ə; Albanian:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algeria (; Arabic: الجزائر‎ al-Jazā'ir, famila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra ( ( listen); Catalan: [ənˈdorə], local...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country                                        description\n",
       "0  Afghanistan  Afghanistan ( ( listen); Pashto/Dari: افغانستا...\n",
       "1      Albania  Albania ( ( listen) a(w)l-BAY-nee-ə; Albanian:...\n",
       "2      Algeria  Algeria (; Arabic: الجزائر‎ al-Jazā'ir, famila...\n",
       "3      Andorra  Andorra ( ( listen); Catalan: [ənˈdorə], local..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('country_desc.csv')\n",
    "print(\"Shape: \", df.shape)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a total of 195 countries and their description. The text also has a few foreign characters such as \"الجزائر al-Jazā'ir\". Such characters are tough to be understood by the neural network unless provided a large number of similar language.\n",
    "\n",
    "We ultimately want only the description for training the model. One of the best approach is to have a text file that has all the descriptions. In the following cells, we will clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE TEXT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Cayman Islands ( or ) is an autonomous British Overseas Territory in the western Caribbean Sea. The 264-square-kilometre (102-square-mile) territory comprises the three islands of Grand Cayman, Cayman Brac and Little Cayman located south of Cuba, northeast of Costa Rica, north of Panama, east of Mexico and northwest of Jamaica. Its population is approximately 60,765, and its capital is George Town.\\r\\nThe Cayman Islands is considered to be part of the geographic Western Caribbean Zone as well as the Greater Antilles. The territory is often considered a major world offshore financial haven for international businesses and many wealthy individuals.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## preview of the text description\n",
    "print(\"SAMPLE TEXT\")\n",
    "df['description'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing #1\n",
    "\n",
    "description=df[['description']]     ## ceating a new dataframe with only descriptions\n",
    "\n",
    "## To remove the carriage return(\\r) in the text, we use a string replace function and replace with nothing('')\n",
    "description=description.replace({r'\\n+': ''}, regex=True)\n",
    "\n",
    "## Drop any duplicate descriptions(highly unlikely that this dataset will have any!)\n",
    "description=description.drop_duplicates()\n",
    "\n",
    "## using an astype(string) to convert any leftover integers or other data types in the text\n",
    "description['description']=description['description'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has to be in a format called \"utf-8\" for it to be understood by the system. So the text has to be converted to that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE TEXT\n",
      "Bermuda () is a British Overseas Territory in the North Atlantic Ocean. It is approximately 1,070 km (665 mi) east-southeast of Cape Hatteras, North Carolina; 1,236 km (768 mi) south of Cape Sable Island, Nova Scotia; and 1,759 km (1,093 mi) north of Cuba. The capital city is Hamilton. Bermuda is self-governing, with its own constitution and its own government, which enacts local laws, while the United Kingdom retains responsibility for defence and foreign relations.\r",
      "Bermuda's two largest economic sectors are offshore insurance and reinsurance, and tourism. Bermuda had one of the world's highest GDP per capita for most of the 20th century. Recently, its economic status has been affected by the global recession. The island has a subtropical climate and lies in the hurricane belt and prone to related severe weather; however, it is somewhat protected by a coral reef that surrounds the island and its position at the north of the belt, which limits the direction and severity of approaching storms.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(description)):  ## looping over the entire dataframe\n",
    "    ## first we encode the text to ascii. ASCII stands for American Standard Code for Information Interchange.\n",
    "    ## It has a numerical value for A-Z, a-z, 0-9 and other standard symbols. The following line encodes them to\n",
    "    ## ascii values and then decodes them back to utf-8\n",
    "    \n",
    "    y=description['description'][i].encode(\"ascii\", errors=\"ignore\").decode('utf-8')\n",
    "    \n",
    "    ## Adding the decoded text to the dataframe\n",
    "    description['description'][i]=y\n",
    "\n",
    "print(\"SAMPLE TEXT\")\n",
    "print(description['description'][20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see above, the text does not have any foreign characters. So it is safe to say that the data is clean. Further cleaning can be performed manually by removing text that are too variant from common grammar. For instance, rarely used words or proper nouns that is difficult to spell by the English Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now to save the file as a csv file for reuse later.\n",
    "filename='description_only.csv'\n",
    "description.to_csv(filename, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now saving as a txt file, that can be directly fed into the model.\n",
    "filename='description_text.txt'\n",
    "description.to_csv(filename, header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Vectorizing words\n",
    "\n",
    "**Current Objective:** To encode the characters as integers and store them in a dictionary.\n",
    "\n",
    "The network can only work with numbers abd it does not have the capacity to work with words or letters. So we have toc onvert the letters into numbers that the network can understand. This process is called **Vectorization**. Here we assign a number to evry unique character in the text. \n",
    "\n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making essential imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 543952\n"
     ]
    }
   ],
   "source": [
    "## open the txt file and store it in the variable 'text'\n",
    "\n",
    "with io.open('description_text.txt') as f:\n",
    "    text = f.read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 81\n"
     ]
    }
   ],
   "source": [
    "## Take out unique characters in text using set and store them as a list\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map each character to integer\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "## Map each integer to character. This is the reverse process of the above line. Used to convert predicted\n",
    "## integers to characters\n",
    "\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sentence list\n",
    "\n",
    "Now, we have to create the training data for our LSTM. We create two lists:\n",
    "    1. **sentences:** This list contains the sequences of words (i.e. a list of words) used to train the model,\n",
    "    2. **next_chars:** This list contains the next words for each sequences of the sequences list.\n",
    "    \n",
    "**How it works**: To create the first sequence of characters, we take the 50th first characters in the text. The character number 51 is the next set of this first sequence, and is added to the next_chars list.\n",
    "\n",
    "Then we jump by a step of 1 (step = 1) in the list of characters, to create the second sequence of words and retrieve the second next char.\n",
    "\n",
    "We iterate this job until the end of the list of chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50    ##Number of characters that the LSTM layer looks at a time\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 543902\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create the matrix X and y to be the data inputs of our model:\n",
    "\n",
    "1.  X : the matrix of the following dimensions:\n",
    "\n",
    "    number of sequences,\n",
    "    number of words in sequences,\n",
    "    number of words in the vocabulary.\n",
    "\n",
    "2. y : the matrix of the following dimensions:\n",
    "\n",
    "    number of sequences,\n",
    "    number of words in the vocabulary.\n",
    "     \n",
    "    For each word, we retrieve its index in the vocabulary, and we set to 1 its position in the matrix. X and y are our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Defining helper functions\n",
    "\n",
    "**Current Objective:** To define helper functions that will be used for training and prediction\n",
    "1. sample - to sample an index from a probability array\n",
    "2. on_epoch_end - invoked at end of each epoch. Prints generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not take the words with the highest prediction (or the generation of text will be boring), but we would like to insert some uncertainties, and let the solution, sometime, to pick-up words with less good prediction.\n",
    "Sample() will draw randomly a word from our vocabulary.\n",
    "\n",
    "However, the probability for a word to be drawn will depends directly on its probability to be the next word.\n",
    "\n",
    "In order to tune this probability, we introduce a “temperature” or \"diversity\" to smooth or sharpen its value.\n",
    "\n",
    " - if temperature = 1.0, the probability for a word to be drawn is similar to the probability for the word to be the next one in the sequence (the output of the word prediction model), compared to other words in the dictionary,\n",
    " - if temperature is big (much bigger than 1), the range of probabilities is shorten: the probabilities for all words to be the next one will increase. More variety of words will be picked-up from the vocabulary, because more words will have high probabilities.\n",
    " - if temperature is small (close to 0), small probabilities will be avoided (they will be set to a value closed to 0). Less words will be picked-up from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In on_opech_end(), a preview of the training process can be obtained. After every 2 epochs, the model predicts based on what it has learnt till then. This gives an understanding on how well the model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    if(epoch%2==0):\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "        for diversity in [0.5, 1.0]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = text[start_index:start_index+maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. Defining model\n",
    "\n",
    "**Current Objective:** Define a model for training\n",
    "\n",
    "Here is the architecture for the model for this tutorial:\n",
    "\n",
    "1. Sequential model with LSTM layer of 128 units. \n",
    " - 128 units is fairly a lower number of units. But given that the training set has only 500k characters and they are very diverse, 128 units could do the task. If the data set is huge and the pattern is complex, then a LSTM with larger number of units is required.\n",
    "\n",
    "2. Dropout layer of 0.5; avoids quick divergence\n",
    "3. A dense layer with the number of characters as size(81 in this case)\n",
    "4. Softmax activation layer\n",
    "\n",
    "\n",
    "**Optimizer** The optimizer used is a RMSprop with a learning rate of 0.02.\n",
    " - No specific reason for the choice of RMSprop here. Adam oprimizer can also be used.\n",
    " \n",
    "Then compiling the model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.02)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               107520    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 81)                10449     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 81)                0         \n",
      "=================================================================\n",
      "Total params: 117,969\n",
      "Trainable params: 117,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "checkpoint=ModelCheckpoint('best_model.h5', monitor='loss', mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Training\n",
    "\n",
    "Now we train the model.\n",
    "\n",
    "Here batch size denotes the number of sequences of maxlen that goes into the model at a time.\n",
    "-----> Batch size=512 for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.8373\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t country on mainland South America after Uruguay,\"\n",
      "t country on mainland South America after Uruguay, the the the ending the Frence was the the Sea for independence bec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ting a countries of the southern country is a mintrity and the and the Bankist As and a meritary in the Horin Inlian country in 1996, and the Republic of Countrie the country of war deveral the north and state on the the country in 1996, for the South States resident sector of ground in the in the republic of a west and the is a fe\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t country on mainland South America after Uruguay,\"\n",
      "t country on mainland South America after Uruguay, gic war of by Slating bends purturan to West.In  Socaint, Eurian. East the Vectake uumandes northtencted the century nowe in a to Mevitipe in the Derges decerturly 2018, low compris deporge's pome isaity is Molilid of gar Uin congina porricer in the 7dils, 1899, which nice of European unialst member of the kingdom of populous and independence 373 cariticen Gan Alantly and economic untir sian undi\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.83732, saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.7105\n",
      "\n",
      "Epoch 00002: loss improved from 1.83732 to 1.71047, saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.6660\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" presidential republic consisting of 17 administra\"\n",
      " presidential republic consisting of 17 administratic but 1910 in the Astandan Africa and the maintar country in the 19th century, establish in the warly in the established by a suborteres and several state sender and to the Mastae and the and in 1915. The altheatire demaining a sumbers a there than south is internity, southern the second in 1976 and its one of the and the American in the and the area in the 19th century, senters. The decenting r\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" presidential republic consisting of 17 administra\"\n",
      " presidential republic consisting of 17 administraio, Onetal Septestraes governments.The entrishads orestas in the Capave Islandanca was tonriatit was with nating oo a redented people arelautred minition to a with runebited tress, it highed by followed untrina, the marks south adu, 1994 a growing the first Development and Necically 1968. Bacterrania and culipls Soviet by the precouthint as the as tolard language was lution of at prospelties, its \n",
      "\n",
      "Epoch 00003: loss improved from 1.71047 to 1.66598, saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.6400\n",
      "\n",
      "Epoch 00004: loss improved from 1.66598 to 1.64005, saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.6237\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" conquered by the Muhammad Ali dynasty. Between 18\"\n",
      " conquered by the Muhammad Ali dynasty. Between 1895 and the as the to the 19th century, and the country in 2018, the west of a to a largest on the aborital country became the nation of the ther the south and the resing the and policy for the country and the Capria and into the tapal and territory of the Cani for with became the second country in the to for Nelana and for former and independence and sovereign by the Western Arab constitution of e\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" conquered by the Muhammad Ali dynasty. Between 18\"\n",
      " conquered by the Muhammad Ali dynasty. Between 186 million of Resonenti. 00 permoniced in the Conar hulked proded which largest theringe and Alakok-territory of Yorea. It commencmine past of one crestic stered population to trestor of from of colonist everal nutorating witalanced as thelimilling has a culedled was stongians. The Procan government in January Ically of most landel f sottentially by its republic of orcanon liled for the Republic as\n",
      "\n",
      "Epoch 00005: loss improved from 1.64005 to 1.62366, saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.6074\n",
      "\n",
      "Epoch 00006: loss improved from 1.62366 to 1.60739, saving model to best_model.h5\n",
      "Epoch 7/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5985\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" modern nation state of Greece emerged in 1830 fol\"\n",
      " modern nation state of Greece emerged in 1830 following the south was the 16th to the country and making in the South, the Kano Conting and which state of population of the Partions and a state of the restrous and the Sama is the to the area of the modern republic of with a military population, is the state of the member of the nation of cherition of Prostin and with the period second in the Central Trimes and the and and the Gulf of large in th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" modern nation state of Greece emerged in 1830 fol\"\n",
      " modern nation state of Greece emerged in 1830 follow Africa freesoativen, majorricied (soeignmal, its prossuns and fir by to cunsural consentron and as its indurtrices and a rasting, Aciis language, in alleb country. The Kathirs duss, aver chratuntial represed Amlerisutl, intlemwenal years licres independent country in Othemasoex Oecame.The as unter it to haghed 664 Datoms widest ligencench of Eosetand has the abladited, mils part of falle ethni\n",
      "\n",
      "Epoch 00007: loss improved from 1.60739 to 1.59854, saving model to best_model.h5\n",
      "Epoch 8/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5899\n",
      "\n",
      "Epoch 00008: loss improved from 1.59854 to 1.58993, saving model to best_model.h5\n",
      "Epoch 9/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5807\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ntral location.The Maldives archipelago is located\"\n",
      "ntral location.The Maldives archipelago is located are the independent and the population and island for the in the main under the teart in the to the part of the second of the member of the southern of the remains and neare internation the official regional as the official language is a granting in the official civil and to the established the world to the maintated population of the there and an to the south country. In 1992 to the Central and \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ntral location.The Maldives archipelago is located\"\n",
      "ntral location.The Maldives archipelago is located doatted's sociates of the 62 officially the rard Efrominans to its main but as the northern excintare. South Africa ruly leas  core., the 19th century. Ig Country and west, had defents states common remigonk agthern Trantround. From the talield and population in warr has first population Federal Percent people. The reengl ally gonslion country from depend area is the Gkum-regian centuries to waps\n",
      "\n",
      "Epoch 00009: loss improved from 1.58993 to 1.58074, saving model to best_model.h5\n",
      "Epoch 10/20\n",
      "543902/543902 [==============================] - 227s 417us/step - loss: 1.5771\n",
      "\n",
      "Epoch 00010: loss improved from 1.58074 to 1.57705, saving model to best_model.h5\n",
      "Epoch 11/20\n",
      "543902/543902 [==============================] - 225s 414us/step - loss: 1.5720\n",
      "----- Generating text after Epoch: 10\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ompany of Cecil Rhodes first demarcated the presen\"\n",
      "ompany of Cecil Rhodes first demarcated the presents is the become the the universial in the control of the island of the British most and independence in 1996 to the Andes and the republic and the the European contral officially the Batina and high city is South African Commone as in the disport a million and other in a main and Constituent controlland and the country of the armally was an is a land a population of population of the mentia and a\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ompany of Cecil Rhodes first demarcated the presen\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompany of Cecil Rhodes first demarcated the presences coust was is an to the tear base heg and the most conssural and was incount and the many Nordia governed in African, which later the colvercoely to followed as ethnic country huls to later exper to creation country. ned is are Sea centuried (85 km (1 mi) (201 narrita the Lamm of percontito fulnsal serge. The alforcogal bethe archipelanity.In 1996, as ethnic only of 2010. Andetian cooperation o\n",
      "\n",
      "Epoch 00011: loss improved from 1.57705 to 1.57202, saving model to best_model.h5\n",
      "Epoch 12/20\n",
      "543902/543902 [==============================] - 225s 414us/step - loss: 1.5696\n",
      "\n",
      "Epoch 00012: loss improved from 1.57202 to 1.56964, saving model to best_model.h5\n",
      "Epoch 13/20\n",
      "543902/543902 [==============================] - 225s 413us/step - loss: 1.5641\n",
      "----- Generating text after Epoch: 12\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"a few hundred metres long.A mid-sized country of j\"\n",
      "a few hundred metres long.A mid-sized country of joint is the has a sovereign largest to the republic and the republic of the Southshernation was internation in the first state of the Area to the south and independence to the region to the third years and the most centrine formed expendent the Paning and in the southeast and kingdom to the Germany of the ender country in the early United since of ne in the world, is been in official with in the U\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"a few hundred metres long.A mid-sized country of j\"\n",
      "a few hundred metres long.A mid-sized country of joinefich this (Manizore established bolde Lithumes and Lithoplo Jana  half and almonr in  (Aonartanian Indies Asia as the has was engariaged by a 17 of the tribemed since matering epreatofted oversonattre deninglicy has late corraped by party eleve until the island of Bhuts land year and declared in firing and Jiatamime Sorthchones, the 14th century, Incledumeran aldoers, 19,241,, the west. Swodan\n",
      "\n",
      "Epoch 00013: loss improved from 1.56964 to 1.56408, saving model to best_model.h5\n",
      "Epoch 14/20\n",
      "543902/543902 [==============================] - 225s 414us/step - loss: 1.5614\n",
      "\n",
      "Epoch 00014: loss improved from 1.56408 to 1.56141, saving model to best_model.h5\n",
      "Epoch 15/20\n",
      "543902/543902 [==============================] - 225s 414us/step - loss: 1.5581\n",
      "----- Generating text after Epoch: 14\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"iscovery, Portuguese explorers pioneered maritime \"\n",
      "iscovery, Portuguese explorers pioneered maritime in the republic in the South Africa of the South Asia of the States and the 19th century, the European Soviet country of the Commonwealth and the South Armenia in the South Armenian in the country to the southern Asia of 19430 of the , and the independence and the African maintations, and in the  and the Union and the most visture in the east of the Republic of the Kingdom is Mondomsan and the sou\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"iscovery, Portuguese explorers pioneered maritime \"\n",
      "iscovery, Portuguese explorers pioneered maritime (parl in liofailes by the Nontae than the 3 rats etro.Trassity is the former part of the southern reana, about ranks to the Turdin, wars of the Country is most secondercol electurations and other porigled the world. Aa election Eveast. Cawicts, Republic and Luzedaruan in genulirure, incomalland is an etreses the andiwans, hocoss achiation in the Minader, with union in 1962. Pisillles and aldopaets\n",
      "\n",
      "Epoch 00015: loss improved from 1.56141 to 1.55815, saving model to best_model.h5\n",
      "Epoch 16/20\n",
      "543902/543902 [==============================] - 225s 414us/step - loss: 1.5556\n",
      "\n",
      "Epoch 00016: loss improved from 1.55815 to 1.55556, saving model to best_model.h5\n",
      "Epoch 17/20\n",
      "543902/543902 [==============================] - 230s 423us/step - loss: 1.5608\n",
      "----- Generating text after Epoch: 16\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"the east; and Equatorial Guinea, Gabon and the Rep\"\n",
      "the east; and Equatorial Guinea, Gabon and the Republic of African in the subtrontist far major economic third in the Greeg and European Bara of the denaming South African major exoster southern as its 2017 has the periods of coast vere and prosent and the United Nations of the remained the country in the economy in the the the subvaries of South African end of the the island of re beterrally in population between the Congle state the area by the\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"the east; and Equatorial Guinea, Gabon and the Rep\"\n",
      "the east; and Equatorial Guinea, Gabon and the Republic and leading of northern from the apterars in 1999, 1211.The official reslating the lier to sutthie state has a and min, and virted as Samanus parliamentary in the aldomatele populated of politically presided radely for the Index and the west by a meveat-pervade-eist Aaver, the Oceans Pentertilanese. The nation in the viftement proalled apporter to the ere. In 1989 island chajoraa liter 6 hea\n",
      "\n",
      "Epoch 00017: loss did not improve from 1.55556\n",
      "Epoch 18/20\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5595\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.55556\n",
      "Epoch 19/20\n",
      "543902/543902 [==============================] - 227s 418us/step - loss: 1.5559\n",
      "----- Generating text after Epoch: 18\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nd Christians in the south. Differences in languag\"\n",
      "nd Christians in the south. Differences in language of its country in the United Stated in the Indian part in the Caribbean 19th century, its 1984 and the Aanter of Africa. The country is its to by the area of the Calia to the northeast Council and the northeast, Eastern Community of Mare, the world's the are by a created in 1941. The Netherlands and its regional deing in the Territory and largest sifsorist in the south of the development in the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nd Christians in the south. Differences in languag\"\n",
      "nd Christians in the south. Differences in language in meder and of the mids, British internations in 11.0 k. It, chices of country and President is system. In the 20th thatic dominally prementury (largesticy is 187,20 spoce and selgize on income one led ey vast southeast, is malinglatanted which being Parteran as its recostally by the world and pomerally demember and 4.qa. The 19th century, and and fine at the tadisis to dominateas-Indian of at \n",
      "\n",
      "Epoch 00019: loss did not improve from 1.55556\n",
      "Epoch 20/20\n",
      "543902/543902 [==============================] - 227s 417us/step - loss: 1.5536\n",
      "\n",
      "Epoch 00020: loss improved from 1.55556 to 1.55359, saving model to best_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2bc972a90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,batch_size=512, epochs=20, callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "As we can see from the previews above, the model has done a fairly good job of capturing the pattern between text. The text generated is not exactly like we wanted, but it is in a readable format.\n",
    "\n",
    "Let us try generating text for an entirely imaginary country... **Wakanda**\n",
    "\n",
    "![Wakanda](https://upload.wikimedia.org/wikipedia/en/9/96/Wakanda_in_Black_Panther_teaser_poster.jpeg)\n",
    "\n",
    "Seed text: \"Wakanda is a country in the southern part of Afric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- temperature/diversity: 0.5\n",
      "----- Generating with seed: \"Wakanda is a country in the southern part of Afric\"\n",
      "Wakanda is a country in the southern part of African country in settled by national in south between Presid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent and the nation in 1990 in the action and be a state and a international population of the Congo of the European Athian to the north of the Independence of its the a successive south of the the United Nations in the country is a founding capital is ad conslise and a state of the country has a country is has official independence in the Capital Seration in the 1915 becement reserves and the country. For the to the south of the world in t\n",
      "----- temperature/diversity: 1.0\n",
      "----- Generating with seed: \"Wakanda is a country in the southern part of Afric\"\n",
      "Wakanda is a country in the southern part of African population for shouring the native prossen regions (nater seads of Argentinas geoprote in its wead population has a lands constitution chambly Heugh union, cilic which of Burses and cathereling millions proration in eite is reoiss a Hunguagia, Touemara-and Republic of The yingepigal Portugal social promalling Karitle, was since memres and in its corsising undly mincar by its economy. helnaddoslet Sprede Bigonan century and political of constituation, Appulicali ethnic 9780. Democratic one int\n"
     ]
    }
   ],
   "source": [
    "for diversity in [0.5, 1.0]:\n",
    "    print('----- temperature/diversity:', diversity)\n",
    "    generated = ''\n",
    "    sentence = \"Wakanda is a country in the southern part of Afric\"\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(500):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Infernce** \n",
    "\n",
    "The model has trained pretty well, with readable text and imaginary places like \"Congo of the European Athian\",\"chambly Heugh union\", \"Republic of The yingepigal Portugal\".\n",
    "\n",
    "But the readability decrease when the temperature(randomness increases)\n",
    "\n",
    "Let us train for an additional 10 epochs and see if the performance improves.\n",
    "\n",
    "**Additional training for 10 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5571\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" with Finland losing parts of Karelia, Salla, Kuus\"\n",
      " with Finland losing parts of Karelia, Salla, Kuusia is the country of par"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t of the west of when the Council of constitution in the world and the President Empire, and the Bastarai Part of the population of the island and subse and form and the Iran exporten of the benetial and large in the north and east of the Antillian severy in the north and east of South end of the south of the population of the most political settlers from the to the country\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" with Finland losing parts of Karelia, Salla, Kuus\"\n",
      " with Finland losing parts of Karelia, Salla, Kuushans, the Par. The major used war regions thate joce, verling as. S, the Man-presentured the largest states the west of the north. Spain is the south, in the largest is a centuries the menied Movement (Ultlabi which of the presud was monarchy largume the world, Independent Ocean. Couse of Empire was the Aliwu Safgen democratios, city of Partyive is World is People's territory of K.cank Sider are a\n",
      "\n",
      "Epoch 00001: loss did not improve from 1.55359\n",
      "Epoch 2/10\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 1.5532\n",
      "\n",
      "Epoch 00002: loss improved from 1.55359 to 1.55324, saving model to best_model.h5\n",
      "Epoch 3/10\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.5495\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"zechoslovakia (19181939). A separate (First) Slova\"\n",
      "zechoslovakia (19181939). A separate (First) Slovanian Saman War is a lenging of the world remained by the sector of the later the as has one of the country is a and was official city of African Union of constitutional third the became the country and national resulted by the the several country is the country in the population of the language consist to the largest country is since in 1922, the military country and western The begas expirted by \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"zechoslovakia (19181939). A separate (First) Slova\"\n",
      "zechoslovakia (19181939). A separate (First) Slovakic Nation, with reserciling adtegnest, the countrys, it of Africo, and king the roith first resource and developed of the Warmanoca War. Destabil pitans in World Ileral of which history, has of 2011, as socienc in the desperougos arming 19 custentic alsophave in provoron became ie Urgen official century until auchousag of which and tenroved, of Israems. Native ofleting the centries \"erustic const\n",
      "\n",
      "Epoch 00003: loss improved from 1.55324 to 1.54951, saving model to best_model.h5\n",
      "Epoch 4/10\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.5509\n",
      "\n",
      "Epoch 00004: loss did not improve from 1.54951\n",
      "Epoch 5/10\n",
      "543902/543902 [==============================] - 227s 418us/step - loss: 1.6848\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"e Pacific Ocean to the west, the Caribbean Sea to \"\n",
      "e Pacific Ocean to the west, the Caribbean Sea to the north Afric period and a official political south Salg mainlands of the agreano countries in 1993 and a second republic in the island in the the western under a sepente of the Europe and the first serative of the savating and the south Leace language and its international and the to the European Africa and America and a developing the head and with an including an administration in the north o\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"e Pacific Ocean to the west, the Caribbean Sea to \"\n",
      "e Pacific Ocean to the west, the Caribbean Sea to the south the Asian and the ong Commun Empire the the German, the Europe and its countries an established adadendeter of spatk. Somesh muperatement part economic pericied. A , srainide ditlogrrouded of one a mulie monarchita is Seopanasmad a period became an occupied independence in. Capithinevani act included Haa Hesseman as particity is Europe became 1907. estinaades hamatlatted is spendanie, th\n",
      "\n",
      "Epoch 00005: loss did not improve from 1.54951\n",
      "Epoch 6/10\n",
      "543902/543902 [==============================] - 227s 418us/step - loss: 1.6143\n",
      "\n",
      "Epoch 00006: loss did not improve from 1.54951\n",
      "Epoch 7/10\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.6635\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Mali to the north; Niger to the east; Benin to th\"\n",
      " Mali to the north; Niger to the east; Benin to the third to Mace, in the island of established the Nonaran and Africa and Countries world and a monarchy and it to the world of country of Poland and Liberia, the ther which was compered the 1995, the Guinea. It is a and former and the Second to the republic. It is a state with constitutionally the such largest founding largest seat and formers and 610 million the Constitution and the official the \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Mali to the north; Niger to the east; Benin to th\"\n",
      " Mali to the north; Niger to the east; Benin to the Phavian Ban evelliced the 1980s, Outsia by cocollem government epiters Indaica and Andolay country.Cismen) ruran controp, visingubed in High to the country than purusta in Citious wedely Europe, Altandon tacalitial to is inland the  16,000 distripol of oges of Prontimil ho the world's Temria un Aslinitien, of Guyo Indocusu 2018. The country. Ig the ralood. Movement in monarchy is a countries. No\n",
      "\n",
      "Epoch 00007: loss did not improve from 1.54951\n",
      "Epoch 8/10\n",
      "543902/543902 [==============================] - 228s 418us/step - loss: 1.5818\n",
      "\n",
      "Epoch 00008: loss did not improve from 1.54951\n",
      "Epoch 9/10\n",
      "543902/543902 [==============================] - 227s 417us/step - loss: 3.2126\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"\n",
      "Andorra also called the Principality of the Valle\"\n",
      "\n",
      "Andorra also called the Principality of the Vallenar part ineor:eecXatan/Thetinan on anonstanan anianarr bea. It anerartien heanon the is issn in e setege neris uneari stine medar was ro the the the to the esto laso Mora of maner of the riene remanel anomer the the bethanaiclas on of   the of Camer of Sus eacantitasia 00, en offither por ineatitaits stan mon as ananonl . Thirilat' Afrelaon the recoll stereate sh fotcora anorss the rees of the To\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "Andorra also called the Principality of the Valle\"\n",
      "\n",
      "Andorra also called the Principality of the Valletliberin os boytes the tur Dranialaan enlanane ofomea tlecloudate vorth inioee uneece r eemelina,&onaleron the , ethinthels, denor unianerebarifaniol anomellysimr becass caneromoralice, Bbirerna of ans born in fo:tieccanies fasso as wraol batenios, toienecter to nissris bo thes coassinse of turis   of ute arisadetl ecaxss eov oloster it home tantanassus an+poteterathees Neetiretlestlans maliatatid\n",
      "\n",
      "Epoch 00009: loss did not improve from 1.54951\n",
      "Epoch 10/10\n",
      "543902/543902 [==============================] - 228s 419us/step - loss: 4.9715\n",
      "\n",
      "Epoch 00010: loss did not improve from 1.54951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2bd42ddd8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,batch_size=512, epochs=10, callbacks=[print_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "For 3 epochs, the model trained but then started overfitting and the loss starts increasing. So the ideal stopping point would be at 23 wpochs in this case.\n",
    "\n",
    "Now, let us load the best model and generate text out of it and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"Wakanda is a country in the southern part of Afric\"\n",
      "Wakanda is a country in the southern part of African border of the Nether and and sence in the world and becoming a mining with a southers the southeast member of the "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "independence with the population and west and a one was established population of the constitution of the control of the independence is official centuries constitution by the , and country to independent country in the Independence in the  in the miginal under in the world and the world part of the South Turkic African country is a population of the Albopicar of the most of the s\n"
     ]
    }
   ],
   "source": [
    "generated = ''\n",
    "sentence = \"Wakanda is a country in the southern part of Afric\"\n",
    "generated += sentence\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "for i in range(500):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.5)\n",
    "    next_char = indices_char[next_index]\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad!!!\n",
    "\n",
    "![obama](https://images.frenchly.us/2016/11/not-bad-obama.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "<a id='8'></a>\n",
    "\n",
    "As we can see above, the model was able to grasp most of the features of the original text.With 500k charactes this is pretty good!\n",
    "\n",
    "Here are some cool places generated by the model during training\n",
    " - \"member of the nation of cherition of Prostin\"\n",
    " - \"Warmanoca War\"\n",
    " - \"Astandan Africa\n",
    " - \"Sea for independence\"\n",
    " - \"Alakok-territory of Yorea\"\n",
    " - \"Aonartanian Indies Asia\"\n",
    " - \"northern excintare\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional resources\n",
    "<a id='9'></a>\n",
    "\n",
    "Andrej Karpathy's blog: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "\n",
    "Colah's blog: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
